{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdce50a-e781-40ab-a429-3c2a8985318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import face_recognition\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime, time\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c708650-86f2-4a49-b96e-ea85b6be8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "with open(\"face_recognition_model.pkl\", \"rb\") as model_file:\n",
    "    knn_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"rb\") as encoder_file:\n",
    "    label_encoder = pickle.load(encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b9e864-7545-4077-be92-613a04d41f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load your trained emotion detection model\n",
    "emotion_model = tf.keras.models.load_model(\"emotion_detection_model.h5\")  # Update with your model's path\n",
    "emotion_model.compile(\n",
    "    optimizer=\"adam\",  # Use the same optimizer as during training\n",
    "    loss=\"categorical_crossentropy\",  # Use the same loss function\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "emotion_labels = [\"happy\", \"sad\", \"neutral\", \"angry\", \"surprised\",\"disgusted\",\"fearful\"]  # Update with your model's classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6a9af-bdec-425f-a079-cff0615e60c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendance system is active between 1:30 AM and 2:42 AM.\n"
     ]
    }
   ],
   "source": [
    "# Initialize attendance log\n",
    "attendance = {}\n",
    "\n",
    "# Time window for the attendance system\n",
    "start_time = time(13, 40)\n",
    "end_time = time(22, 58)\n",
    "\n",
    "# Function to detect emotions using the trained model\n",
    "def detect_emotion(face_img):\n",
    "    # Resize face to the input size of your model (e.g., 48x48 or 64x64)\n",
    "    face_resized = cv.resize(face_img, (48, 48))  # Update with your model's input size\n",
    "    face_resized = cv.cvtColor(face_resized, cv.COLOR_BGR2GRAY)  # If your model expects grayscale\n",
    "    face_resized = face_resized / 255.0  # Normalize pixel values\n",
    "    face_resized = np.expand_dims(face_resized, axis=0)  # Add batch dimension\n",
    "    face_resized = np.expand_dims(face_resized, axis=-1)  # Add channel dimension if needed\n",
    "\n",
    "    # Predict emotion\n",
    "    emotion_probs = emotion_model.predict(face_resized)\n",
    "    emotion_index = np.argmax(emotion_probs)\n",
    "    return emotion_labels[emotion_index]\n",
    "\n",
    "# Check if within time window\n",
    "def is_within_time_window(start, end):\n",
    "    current_time = datetime.now().time()\n",
    "    return start <= current_time <= end\n",
    "\n",
    "# Start video capture\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "print(\"Attendance system is active between 1:30 AM and 2:42 AM.\")\n",
    "\n",
    "while True:\n",
    "    # Check if within the time window\n",
    "    if not is_within_time_window(start_time, end_time):\n",
    "        print(\"Attendance system is inactive. Waiting for the next time window.\")\n",
    "        break\n",
    "\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to capture frame. Exiting.\")\n",
    "        break\n",
    "\n",
    "    # Resize for faster processing\n",
    "    imgS = cv.resize(img, (0, 0), None, 0.25, 0.25)\n",
    "    imgS = cv.cvtColor(imgS, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces\n",
    "    face_locations = face_recognition.face_locations(imgS)\n",
    "    face_encodings = face_recognition.face_encodings(imgS, face_locations)\n",
    "\n",
    "    for face_encoding, face_loc in zip(face_encodings, face_locations):\n",
    "        # Predict student ID\n",
    "        predictions = knn_model.kneighbors([face_encoding], n_neighbors=1)\n",
    "        match_index = predictions[1][0][0]\n",
    "        student_id = label_encoder.inverse_transform([match_index])[0]\n",
    "\n",
    "        if student_id not in attendance:\n",
    "            # Mark attendance as \"Present\"\n",
    "            y1, x2, y2, x1 = face_loc\n",
    "            y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4\n",
    "            face_img = img[y1:y2, x1:x2]\n",
    "\n",
    "            # Detect emotion\n",
    "            emotion = detect_emotion(face_img)\n",
    "\n",
    "            # Update attendance\n",
    "            attendance[student_id] = {\n",
    "                \"Time\": datetime.now().strftime(\"%H:%M:%S\"),\n",
    "                \"Emotion\": emotion,\n",
    "                \"Status\": \"Present\"\n",
    "            }\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv.putText(img, f\"{student_id} ({emotion})\", (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the video\n",
    "    cv.imshow(\"Attendance System\", img)\n",
    "\n",
    "    # Quit with 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Check for any students who didn't get marked as \"Present\"\n",
    "for student_id in label_encoder.classes_:\n",
    "    if student_id not in attendance:\n",
    "        attendance[student_id] = {\n",
    "            \"Time\": \"Not detected\",\n",
    "            \"Emotion\": \"N/A\",\n",
    "            \"Status\": \"Absent\"\n",
    "        }\n",
    "\n",
    "# Save attendance to CSV\n",
    "attendance_df = pd.DataFrame.from_dict(attendance, orient=\"index\")\n",
    "attendance_df.index.name = \"Student ID\"\n",
    "attendance_df.to_csv(\"Attendance.csv\")\n",
    "print(\"Attendance saved to Attendance.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c273e25-5d66-4569-80f1-ff2a643139ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
